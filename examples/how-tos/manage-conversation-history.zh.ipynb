{"cells": [{"cell_type": "markdown", "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53", "metadata": {}, "source": ["# 如何管理对话历史记录\n", "\n", "持久性最常见的用例之一是用它来跟踪对话历史记录。这太棒了——它让继续对话变得容易。然而，随着对话变得越来越长，此对话历史记录可能会累积并占用越来越多的上下文窗口。这通常是不可取的，因为它会导致对 LLM 的调用更加昂贵和耗时，并且可能会出现错误。为了防止这种情况发生，您可能需要管理对话历史记录。\n", "\n", "注意：本指南重点介绍如何在 LangGraph 中执行此操作，您可以在其中完全自定义执行此操作的方式。如果您想要更现成的解决方案，您可以查看 LangChain 中提供的功能：\n", "\n", "- [如何过滤消息](https://js.langchain.com/docs/how_to/filter_messages/)\n", "- [如何修剪消息](https://js.langchain.com/docs/how_to/trim_messages/)"]}, {"cell_type": "markdown", "id": "7cbd446a-808f-4394-be92-d45ab818953c", "metadata": {}, "source": ["## 设置\n", "\n", "首先，让我们设置我们要使用的包\n", "```bash\n", "yarn add langchain @langchain/anthropic @langchain/core\n", "```\n", "接下来，我们需要为 Anthropic 设置 API 密钥（我们将使用的 LLM）\n", "```bash\n", "export ANTHROPIC_API_KEY=your_api_key\n", "```"]}, {"cell_type": "markdown", "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c", "metadata": {}, "source": ["或者，我们可以为 [LangSmith 追踪](https://smith.langchain.com/) 设置 API 密钥，这将为我们提供一流的可观察性。\n", "```bash\n", "export LANGCHAIN_TRACING_V2=\"true\"\n", "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n", "export LANGCHAIN_API_KEY=your_api_key\n", "```"]}, {"cell_type": "markdown", "id": "4767ef1c-a7cf-41f8-a301-558988cb7ac5", "metadata": {}, "source": ["## 构建代理\n", "现在让我们构建一个简单的 ReAct 风格代理。"]}, {"cell_type": "code", "execution_count": 2, "id": "378899a9-3b9a-4748-95b6-eb00e0828677", "metadata": {}, "outputs": [], "source": ["import { ChatAnthropic } from \"@langchain/anthropic\";\n", "import { tool } from \"@langchain/core/tools\";\n", "import { BaseMessage, AIMessage } from \"@langchain/core/messages\";\n", "import { StateGraph, Annotation, START, END } from \"@langchain/langgraph\";\n", "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n", "import { MemorySaver } from \"@langchain/langgraph\";\n", "import { z } from \"zod\";\n", "\n", "const AgentState = Annotation.Root({\n", "  messages: Annotation<BaseMessage[]>({\n", "    reducer: (x, y) => x.concat(y),\n", "  }),\n", "});\n", "\n", "const memory = new MemorySaver();\n", "\n", "const searchTool = tool((_): string => {\n", "    // 这是实际实现的占位符\n", "    // 不过不要让LLM知道这一点😊\n", "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\"\n", "}, {\n", "    name: \"search\",\n", "    description: \"Call to surf the web.\",\n", "    schema: z.object({\n", "        query: z.string()\n", "    })\n", "})\n", "\n", "\n", "const tools = [searchTool]\n", "const toolNode = new ToolNode<typeof AgentState.State>(tools)\n", "const model = new ChatAnthropic({ model: \"claude-3-haiku-20240307\" })\n", "const boundModel = model.bindTools(tools)\n", "\n", "function shouldContinue(state: typeof AgentState.State): \"action\" | typeof END {\n", "  const lastMessage = state.messages[state.messages.length - 1];\n", "  // 如果没有函数调用，那么我们就完成了\n", "  if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) {\n", "      return END;\n", "  }\n", "  // 否则，如果有，我们继续\n", "  return \"action\";\n", "}\n", "\n", "// 定义调用模型的函数\n", "async function callModel(state: typeof AgentState.State) {\n", "  const response = await model.invoke(state.messages);\n", "  // 我们返回一个对象，因为这将与现有状态合并\n", "  return { messages: [response] };\n", "}\n", "\n", "// 定义一个新图\n", "const workflow = new StateGraph(AgentState)\n", "    // 定义我们将在其之间循环的两个节点\n", "    .addNode(\"agent\", callModel)\n", "    .addNode(\"action\", toolNode)\n", "    // 我们现在添加一个条件边\n", "    .addConditionalEdges(\n", "        // 首先，我们定义起始节点。我们使用“代理”。\n", "        // 这意味着这些是调用“agent”节点后获取的边。\n", "        \"agent\",\n", "        // 接下来，我们传入将确定接下来调用哪个节点的函数。\n", "        shouldContinue\n", "    )\n", "    // 我们现在添加从“action”到“agent”的正常边缘。\n", "    // 这意味着在调用“action”之后，接下来调用“agent”节点。\n", "    .addEdge(\"action\", \"agent\")\n", "    // 将入口点设置为“agent”\n", "    // 这意味着该节点是第一个被调用的节点\n", "    .addEdge(START, \"agent\");\n", "\n", "// 最后，我们编译它！\n", "// 这会将其编译成 LangChain Runnable，\n", "// 这意味着您可以像使用任何其他可运行程序一样使用它\n", "const app = workflow.compile({\n", "    checkpointer: memory,\n", "});"]}, {"cell_type": "code", "execution_count": 3, "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["================================ human Message (1) =================================\n", "hi! I'm bob\n", "================================ ai Message (1) =================================\n", "Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you may have. Please let me know if there's anything I can assist you with.\n", "\n", "\n", "================================= END =================================\n", "\n", "\n", "================================ human Message (2) =================================\n", "what's my name?\n", "================================ ai Message (2) =================================\n", "Your name is Bob, as you introduced yourself earlier.\n"]}], "source": ["import { HumanMessage } from \"@langchain/core/messages\";\n", "\n", "const config = { configurable: { thread_id: \"2\"}, streamMode: \"values\" as const }\n", "\n", "const inputMessage = new HumanMessage(\"hi! I'm bob\");\n", "for await (const event of await app.stream({\n", "    messages: [inputMessage]\n", "}, config)) {\n", "    const recentMsg = event.messages[event.messages.length - 1];\n", "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n", "    console.log(recentMsg.content);\n", "}\n", "\n", "console.log(\"\\n\\n================================= END =================================\\n\\n\")\n", "\n", "const inputMessage2 = new HumanMessage(\"what's my name?\");\n", "for await (const event of await app.stream({\n", "    messages: [inputMessage2]\n", "}, config)) {\n", "    const recentMsg = event.messages[event.messages.length - 1];\n", "    console.log(`================================ ${recentMsg._getType()} Message (2) =================================`)\n", "    console.log(recentMsg.content);\n", "}\n"]}, {"cell_type": "markdown", "id": "5d5da4c9-ba8b-46cb-a860-63fe585d15c5", "metadata": {}, "source": ["## 过滤消息\n", "\n", "防止对话历史记录爆炸的最直接的方法是在消息传递给法学硕士之前过滤消息列表。这涉及两个部分：定义一个函数来过滤消息，然后将其添加到图中。请参阅下面的示例，它定义了一个非常简单的 `filterMessages` 函数，然后使用它。"]}, {"cell_type": "code", "execution_count": 4, "id": "eb20430f", "metadata": {}, "outputs": [], "source": ["import { ChatAnthropic } from \"@langchain/anthropic\";\n", "import { tool } from \"@langchain/core/tools\";\n", "import { BaseMessage, AIMessage } from \"@langchain/core/messages\";\n", "import { StateGraph, Annotation, START, END } from \"@langchain/langgraph\";\n", "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n", "import { MemorySaver } from \"@langchain/langgraph\";\n", "import { z } from \"zod\";\n", "\n", "const MessageFilteringAgentState = Annotation.Root({\n", "  messages: Annotation<BaseMessage[]>({\n", "    reducer: (x, y) => x.concat(y),\n", "  }),\n", "});\n", "\n", "const messageFilteringMemory = new MemorySaver();\n", "\n", "const messageFilteringSearchTool = tool((_): string => {\n", "    // 这是实际实现的占位符\n", "    // 不过不要让LLM知道这一点😊\n", "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini 😈.\"\n", "}, {\n", "    name: \"search\",\n", "    description: \"Call to surf the web.\",\n", "    schema: z.object({\n", "        query: z.string()\n", "    })\n", "})\n", "\n", "// 我们可以重复使用与上面相同的搜索工具，因为我们不需要在此示例中对其进行更改。\n", "const messageFilteringTools = [messageFilteringSearchTool]\n", "const messageFilteringToolNode = new ToolNode<typeof MessageFilteringAgentState.State>(messageFilteringTools)\n", "const messageFilteringModel = new ChatAnthropic({ model: \"claude-3-haiku-20240307\" })\n", "const boundMessageFilteringModel = messageFilteringModel.bindTools(messageFilteringTools)\n", "\n", "\n", "async function shouldContinueMessageFiltering(state: typeof MessageFilteringAgentState.State): Promise<\"action\" | typeof END> {\n", "    const lastMessage = state.messages[state.messages.length - 1];\n", "    // 如果没有函数调用，那么我们就完成了\n", "    if (lastMessage && !(lastMessage as AIMessage).tool_calls?.length) {\n", "        return END;\n", "    }\n", "    // 否则，如果有，我们继续\n", "    return \"action\";\n", "}\n", "\n", "const filterMessages = (messages: BaseMessage[]): BaseMessage[] => {\n", "  // 这是非常简单的辅助函数，仅使用最后一条消息\n", "  return messages.slice(-1);\n", "}\n", "\n", "// 定义调用模型的函数\n", "async function callModelMessageFiltering(state: typeof MessageFilteringAgentState.State) {\n", "  const response = await boundMessageFilteringModel.invoke(filterMessages(state.messages));\n", "  // 我们返回一个对象，因为这将与现有状态合并\n", "  return { messages: [response] };\n", "}\n", "\n", "\n", "// 定义一个新图\n", "const messageFilteringWorkflow = new StateGraph(MessageFilteringAgentState)\n", "  // 定义我们将在其之间循环的两个节点\n", "  .addNode(\"agent\", callModelMessageFiltering)\n", "  .addNode(\"action\", messageFilteringToolNode)\n", "  // 我们现在添加一个条件边\n", "  .addConditionalEdges(\n", "    // 首先，我们定义起始节点。我们使用“代理”。\n", "    // 这意味着这些是调用“agent”节点后获取的边。\n", "    \"agent\",\n", "    // 接下来，我们传入将确定接下来调用哪个节点的函数。\n", "    shouldContinueMessageFiltering\n", "  )\n", "  // 我们现在添加从“action”到“agent”的正常边缘。\n", "  // 这意味着在调用“action”之后，接下来调用“agent”节点。\n", "  .addEdge(\"action\", \"agent\")\n", "  // 将入口点设置为“agent”\n", "  // 这意味着该节点是第一个被调用的节点\n", "  .addEdge(START, \"agent\");\n", "\n", "// 最后，我们编译它！\n", "// 这会将其编译成 LangChain Runnable，\n", "// 这意味着您可以像使用任何其他可运行程序一样使用它\n", "const messageFilteringApp = messageFilteringWorkflow.compile({\n", "    checkpointer: messageFilteringMemory,\n", "});"]}, {"cell_type": "code", "execution_count": 5, "id": "52468ebb-4b23-45ac-a98e-b4439f37740a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["================================ human Message (1) =================================\n", "hi! I'm bob\n", "================================ ai Message (1) =================================\n", "Hello, nice to meet you Bob! I'm an AI assistant here to help out. Feel free to let me know if you have any questions or if there's anything I can assist with.\n", "\n", "\n", "================================= END =================================\n", "\n", "\n", "================================ human Message (2) =================================\n", "what's my name?\n", "================================ ai Message (2) =================================\n", "I'm afraid I don't actually know your name, since you haven't provided that information to me. As an AI assistant, I don't have access to personal details about you unless you share them with me directly. I'm happy to continue our conversation, but I don't have enough context to know your specific name. Please feel free to introduce yourself if you'd like me to address you by name.\n"]}], "source": ["import { HumanMessage } from \"@langchain/core/messages\";\n", "\n", "const messageFilteringConfig = { configurable: { thread_id: \"2\"}, streamMode: \"values\" as const }\n", "\n", "const messageFilteringInput = new HumanMessage(\"hi! I'm bob\");\n", "for await (const event of await messageFilteringApp.stream({\n", "    messages: [messageFilteringInput]\n", "}, messageFilteringConfig)) {\n", "    const recentMsg = event.messages[event.messages.length - 1];\n", "    console.log(`================================ ${recentMsg._getType()} Message (1) =================================`)\n", "    console.log(recentMsg.content);\n", "}\n", "\n", "console.log(\"\\n\\n================================= END =================================\\n\\n\")\n", "\n", "const messageFilteringInput2 = new HumanMessage(\"what's my name?\");\n", "for await (const event of await messageFilteringApp.stream(\n", "  {\n", "    messages: [messageFilteringInput2]\n", "  },\n", "  messageFilteringConfig\n", ")) {\n", "    const recentMsg = event.messages[event.messages.length - 1];\n", "    console.log(`================================ ${recentMsg._getType()} Message (2) =================================`)\n", "    console.log(recentMsg.content);\n", "}"]}, {"cell_type": "markdown", "id": "454102b6-7112-4710-aa08-ba675e8be14c", "metadata": {}, "source": ["在上面的例子中，我们自己定义了 `filter_messages` 函数。我们还提供现成的方法来修剪和过滤 LangChain 中的消息。\n", "\n", "- [如何过滤消息](https://js.langchain.com/docs/how_to/filter_messages/)\n", "- [如何修剪消息](https://js.langchain.com/docs/how_to/trim_messages/)"]}], "metadata": {"kernelspec": {"display_name": "TypeScript", "language": "typescript", "name": "tslab"}, "language_info": {"codemirror_mode": {"mode": "typescript", "name": "javascript", "typescript": true}, "file_extension": ".ts", "mimetype": "text/typescript", "name": "typescript", "version": "3.7.2"}}, "nbformat": 4, "nbformat_minor": 5}