{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 如何从头开始创建 ReAct 代理（函数式 API）\n", "\n", "!!!信息“先决条件”\n", "本指南假设您熟悉以下内容：\n", "\n", "- [聊天模型](https://js.langchain.com/docs/concepts/chat_models)\n", "- [消息](https://js.langchain.com/docs/concepts/messages)\n", "- [工具调用](https://js.langchain.com/docs/concepts/tool_calling/)\n", "- [入口点](../../concepts/tical_api/#entrypoint) 和 [任务](../../concepts/tical_api/#task)\n", "\n", "本指南演示如何使用 LangGraph [Functional API](../../concepts/functional_api) 实现 ReAct 代理。\n", "\n", "ReAct 代理是一个[工具调用代理](../../concepts/agentic_concepts/#tool-calling-agent)，其操作如下：\n", "\n", "1.向聊天模型发出查询；\n", "2. 如果模型没有生成[工具调用](../../concepts/agentic_concepts/#tool-calling)，我们返回模型响应。\n", "3. 如果模型生成工具调用，我们使用可用工具执行工具调用，将它们作为[工具消息](https://js.langchain.com/docs/concepts/messages/)附加到我们的消息列表中，然后重复该过程。\n", "\n", "这是一种简单且多功能的设置，可以通过内存、人机交互功能和其他功能进行扩展。有关示例，请参阅专用的[操作指南](../../how-tos/#prebuilt-react-agent)。"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置\n", "\n", "!!!注意兼容性\n", "\n", "本指南需要 `@langchain/langgraph>=0.2.42`。\n", "\n", "首先，安装本示例所需的依赖项：\n", "```bash\n", "npm install @langchain/langgraph @langchain/openai @langchain/core zod\n", "```\n", "接下来，我们需要为 OpenAI 设置 API 密钥（我们将使用的 LLM）：\n", "```typescript\n", "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n", "```\n", "!!!提示“为 LangGraph 开发设置 [LangSmith](https://smith.langchain.com)”\n", "\n", "注册 LangSmith 以快速发现问题并提高 LangGraph 项目的性能。LangSmith 允许您使用跟踪数据来调试、测试和监控使用 LangGraph 构建的 LLM 应用程序 — 在[此处](https://docs.smith.langchain.com)了解有关如何开始的更多信息"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 创建ReAct代理\n", "\n", "现在您已经安装了所需的软件包并设置了环境变量，我们可以创建代理。\n", "\n", "### 定义模型和工具\n", "\n", "让我们首先定义我们将用于示例的工具和模型。在这里，我们将使用一个占位符工具来获取某个位置的天气描述。\n", "\n", "在本示例中，我们将使用 [OpenAI](https://js.langchain.com/docs/integrations/providers/openai/) 聊天模型，但任何[支持工具调用](https://js.langchain.com/docs/integrations/chat/) 的模型就足够了。"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import { ChatOpenAI } from \"@langchain/openai\";\n", "import { tool } from \"@langchain/core/tools\";\n", "import { z } from \"zod\";\n", "\n", "const model = new ChatOpenAI({\n", "  model: \"gpt-4o-mini\",\n", "});\n", "\n", "const getWeather = tool(async ({ location }) => {\n", "  const lowercaseLocation = location.toLowerCase();\n", "  if (lowercaseLocation.includes(\"sf\") || lowercaseLocation.includes(\"san francisco\")) {\n", "    return \"It's sunny!\";\n", "  } else if (lowercaseLocation.includes(\"boston\")) {\n", "    return \"It's rainy!\";\n", "  } else {\n", "    return `I am not sure what the weather is in ${location}`;\n", "  }\n", "}, {\n", "  name: \"getWeather\",\n", "  schema: z.object({\n", "    location: z.string().describe(\"location to get the weather for\"),\n", "  }),\n", "  description: \"Call to get the weather from a specific location.\"\n", "});\n", "\n", "const tools = [getWeather];"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 定义任务\n", "\n", "接下来我们定义我们将执行的 [tasks](../../concepts/tical_api/#task)。这里有两个不同的任务：\n", "\n", "1. **调用模型**：我们想要使用消息列表查询我们的聊天模型。\n", "2. **调用工具**：如果我们的模型生成工具调用，我们想要执行它们。"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import {\n", "  type BaseMessageLike,\n", "  AIMessage,\n", "  ToolMessage,\n", "} from \"@langchain/core/messages\";\n", "import { type ToolCall } from \"@langchain/core/messages/tool\";\n", "import { task } from \"@langchain/langgraph\";\n", "\n", "const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\n", "\n", "const callModel = task(\"callModel\", async (messages: BaseMessageLike[]) => {\n", "  const response = await model.bindTools(tools).invoke(messages);\n", "  return response;\n", "});\n", "\n", "const callTool = task(\n", "  \"callTool\",\n", "  async (toolCall: ToolCall): Promise<AIMessage> => {\n", "    const tool = toolsByName[toolCall.name];\n", "    const observation = await tool.invoke(toolCall.args);\n", "    return new ToolMessage({ content: observation, tool_call_id: toolCall.id });\n", "    // 也可以直接将toolCall传递到工具中以返回ToolMessage\n", "    // 返回 tool.invoke(toolCall);\n", "  });"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 定义入口点\n", "\n", "我们的[entrypoint](../../concepts/tical_api/#entrypoint)将处理这两个任务的编排。如上所述，当我们的 `callModel` 任务生成工具调用时，`callTool` 任务将为每个工具生成响应。我们将所有消息附加到单个消息列表中。"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import { entrypoint, addMessages } from \"@langchain/langgraph\";\n", "\n", "const agent = entrypoint(\n", "  \"agent\",\n", "  async (messages: BaseMessageLike[]) => {\n", "    let currentMessages = messages;\n", "    let llmResponse = await callModel(currentMessages);\n", "    while (true) {\n", "      if (!llmResponse.tool_calls?.length) {\n", "        break;\n", "      }\n", "\n", "      // 执行工具\n", "      const toolResults = await Promise.all(\n", "        llmResponse.tool_calls.map((toolCall) => {\n", "          return callTool(toolCall);\n", "        })\n", "      );\n", "      \n", "      // 附加到消息列表\n", "      currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]);\n", "\n", "      // 再次调用模型\n", "      llmResponse = await callModel(currentMessages);\n", "    }\n", "\n", "    return llmResponse;\n", "  }\n", ");"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 用法\n", "\n", "要使用我们的代理，我们使用消息列表来调用它。根据我们的实现，这些可以是 LangChain [消息](https://js.langchain.com/docs/concepts/messages/) 对象或 OpenAI 风格的对象："]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{ role: 'user', content: \"What's the weather in san francisco?\" }\n", "\n", "callModel:\n", "============================== ai message ==============================\n", "\n", "[\n", "  {\n", "    \"name\": \"getWeather\",\n", "    \"args\": {\n", "      \"location\": \"San Francisco\"\n", "    },\n", "    \"type\": \"tool_call\",\n", "    \"id\": \"call_m5jZoH1HUtH6wA2QvexOHutj\"\n", "  }\n", "]\n", "\n", "callTool:\n", "============================== tool message ==============================\n", "It's sunny!\n", "\n", "callModel:\n", "============================== ai message ==============================\n", "The weather in San Francisco is sunny!\n"]}], "source": ["import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n", "\n", "const prettyPrintMessage = (message: BaseMessage) => {\n", "  console.log(\"=\".repeat(30), `${message.getType()} message`, \"=\".repeat(30));\n", "  console.log(message.content);\n", "  if (isAIMessage(message) && message.tool_calls?.length) {\n", "    console.log(JSON.stringify(message.tool_calls, null, 2));\n", "  }\n", "}\n", "\n", "// 使用示例\n", "const userMessage = { role: \"user\", content: \"What's the weather in san francisco?\" };\n", "console.log(userMessage);\n", "\n", "const stream = await agent.stream([userMessage]);\n", "\n", "for await (const step of stream) {\n", "  for (const [taskName, update] of Object.entries(step)) {\n", "    const message = update as BaseMessage;\n", "    // 仅打印任务更新\n", "    if (taskName === \"agent\") continue;\n", "    console.log(`\\n${taskName}:`);\n", "    prettyPrintMessage(message);\n", "  }\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["完美的！该图正确调用`getWeather`工具，并在收到工具发送的信息后响应用户。查看 LangSmith 跟踪[此处](https://smith.langchain.com/public/8132d3b8-2c91-40fc-b660-b766ca33e9cb/r)。"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 添加线程级持久化\n", "\n", "添加[线程级持久性](../../concepts/persistence#threads) 让我们支持与代理的对话体验：后续调用将附加到先前的消息列表，保留完整的对话上下文。\n", "\n", "向我们的代理添加线程级持久性：\n", "\n", "1. 选择一个[checkpointer](../../concepts/persistence#checkpointer-libraries)：这里我们将使用[MemorySaver](/langgraphjs/reference/classes/checkpoint.MemorySaver.html)，一个简单的内存检查指针。\n", "2. 更新我们的入口点以接受先前的消息状态作为第二个参数。在这里，我们只是将消息更新附加到之前的消息序列中。\n", "3. 选择将从工作流程返回哪些值以及由检查点保存哪些值。如果我们从 `entrypoint.final` 返回它，我们将能够以 `getPreviousState()` 的形式访问它（可选）"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["import {\n", "  MemorySaver,\n", "  getPreviousState,\n", "} from \"@langchain/langgraph\";\n", "\n", "// 突出显示下一行\n", "const checkpointer = new MemorySaver();\n", "\n", "const agentWithMemory = entrypoint({\n", "  name: \"agentWithMemory\",\n", "  // 突出显示下一行\n", "  checkpointer,\n", "}, async (messages: BaseMessageLike[]) => {\n", "  const previous = getPreviousState<BaseMessage>() ?? [];\n", "  let currentMessages = addMessages(previous, messages);\n", "  let llmResponse = await callModel(currentMessages);\n", "  while (true) {\n", "    if (!llmResponse.tool_calls?.length) {\n", "      break;\n", "    }\n", "\n", "    // 执行工具\n", "    const toolResults = await Promise.all(\n", "      llmResponse.tool_calls.map((toolCall) => {\n", "        return callTool(toolCall);\n", "      })\n", "    );\n", "    \n", "    // 附加到消息列表\n", "    currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]);\n", "\n", "    // 再次调用模型\n", "    llmResponse = await callModel(currentMessages);\n", "  }\n", "  \n", "  // 附加最终响应以进行存储\n", "  currentMessages = addMessages(currentMessages, llmResponse);\n", "\n", "  // 突出显示下一行\n", "  return entrypoint.final({\n", "    // 突出显示下一行\n", "    value: llmResponse,\n", "    // 突出显示下一行\n", "    save: currentMessages,\n", "    // 突出显示下一行\n", "  });\n", "});"]}, {"cell_type": "markdown", "metadata": {}, "source": ["现在，我们需要在运行应用程序时传递配置。该配置将为会话线程指定一个标识符。\n", "\n", "!!!提示\n", "\n", "在我们的[概念页面](../../concepts/persistence/) 和[操作指南](../../how-tos/#persistence) 中了解有关线程级持久性的更多信息。"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["const config = { configurable: { thread_id: \"1\" } };"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们以与之前相同的方式启动一个线程，这次传入配置："]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "callModel:\n", "============================== ai message ==============================\n", "\n", "[\n", "  {\n", "    \"name\": \"getWeather\",\n", "    \"args\": {\n", "      \"location\": \"san francisco\"\n", "    },\n", "    \"type\": \"tool_call\",\n", "    \"id\": \"call_4vaZqAxUabthejqKPRMq0ngY\"\n", "  }\n", "]\n", "\n", "callTool:\n", "============================== tool message ==============================\n", "It's sunny!\n", "\n", "callModel:\n", "============================== ai message ==============================\n", "The weather in San Francisco is sunny!\n"]}], "source": ["const streamWithMemory = await agentWithMemory.stream([{\n", "  role: \"user\",\n", "  content: \"What's the weather in san francisco?\",\n", "}], config);\n", "\n", "for await (const step of streamWithMemory) {\n", "  for (const [taskName, update] of Object.entries(step)) {\n", "    const message = update as BaseMessage;\n", "    // 仅打印任务更新\n", "    if (taskName === \"agentWithMemory\") continue;\n", "    console.log(`\\n${taskName}:`);\n", "    prettyPrintMessage(message);\n", "  }\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["当我们询问后续对话时，模型使用先前的上下文来推断我们正在询问天气："]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "callModel:\n", "============================== ai message ==============================\n", "\n", "[\n", "  {\n", "    \"name\": \"getWeather\",\n", "    \"args\": {\n", "      \"location\": \"boston, ma\"\n", "    },\n", "    \"type\": \"tool_call\",\n", "    \"id\": \"call_YDrNfZr5XnuBBq5jlIXaxC5v\"\n", "  }\n", "]\n", "\n", "callTool:\n", "============================== tool message ==============================\n", "It's rainy!\n", "\n", "callModel:\n", "============================== ai message ==============================\n", "In comparison, while San Francisco is sunny, Boston, MA is experiencing rain.\n"]}], "source": ["const followupStreamWithMemory = await agentWithMemory.stream([{\n", "  role: \"user\",\n", "  content: \"How does it compare to Boston, MA?\",\n", "}], config);\n", "\n", "for await (const step of followupStreamWithMemory) {\n", "  for (const [taskName, update] of Object.entries(step)) {\n", "    const message = update as BaseMessage;\n", "    // 仅打印任务更新\n", "    if (taskName === \"agentWithMemory\") continue;\n", "    console.log(`\\n${taskName}:`);\n", "    prettyPrintMessage(message);\n", "  }\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["在 [LangSmith 跟踪](https://smith.langchain.com/public/ec803712-ecfc-49b6-8f54-92252d1e5e33/r) 中，我们可以看到每个模型调用中都保留了完整的对话上下文。"]}], "metadata": {"kernelspec": {"display_name": "TypeScript", "language": "typescript", "name": "tslab"}, "language_info": {"codemirror_mode": {"mode": "typescript", "name": "javascript", "typescript": true}, "file_extension": ".ts", "mimetype": "text/typescript", "name": "typescript", "version": "3.7.2"}}, "nbformat": 4, "nbformat_minor": 4}