{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "d2eecb96-cf0e-47ed-8116-88a7eaa4236d", "metadata": {}, "source": ["# 如何添加跨线程持久化（函数式API）\n", "\n", "!!!信息“先决条件”\n", "\n", "本指南假设您熟悉以下内容：\n", "\n", "- [功能 API](../../concepts/function_api/)\n", "- [持久性](../../概念/持久性/)\n", "- [记忆](../../概念/记忆/)\n", "- [聊天模型](https://js.langchain.com/docs/concepts/chat_models/)\n", "\n", "LangGraph 允许您跨**不同的[线程](../../concepts/persistence/#threads)** 保存数据。例如，您可以将有关用户的信息（他们的姓名或首选项）存储在共享（跨线程）内存中，并在新线程（例如新对话）中重用它们。\n", "\n", "使用[功能API](../../concepts/function_api/)时，您可以使用[Store](/langgraphjs/reference/classes/checkpoint.BaseStore.html)接口将其设置为存储和检索内存：\n", "\n", "1. 创建一个 `Store` 的实例\n", "    ```ts\n", "    import { InMemoryStore } from \"@langchain/langgraph\";\n", "    \n", "    const store = new InMemoryStore();\n", "    ```\n", "2. 将 `store` 实例传递给 `entrypoint()` 包装函数。它将作为 `config.store` 传递到工作流程。\n", "    ```ts\n", "    import { entrypoint } from \"@langchain/langgraph\";\n", "    \n", "    const workflow = entrypoint({\n", "      store,\n", "      name: \"myWorkflow\",\n", "    }, async (input, config) => {\n", "      const foo = await myTask({input, store: config.store});\n", "      ...\n", "    });\n", "    ```\n", "在本指南中，我们将展示如何构建和使用具有使用 [Store](/langgraphjs/reference/classes/checkpoint.BaseStore.html) 接口实现的共享内存的工作流程。\n", "\n", "!!!提示“注意”\n", "\n", "如果您需要向 `StateGraph` 添加跨线程持久性，请查看此[操作指南](../cross-thread-persistence)。"]}, {"cell_type": "markdown", "id": "153fceff", "metadata": {}, "source": ["## 设置\n", "\n", "!!!注意兼容性\n", "\n", "本指南需要 `@langchain/langgraph>=0.2.42`。\n", "\n", "首先，安装本示例所需的依赖项：\n", "```bash\n", "npm install @langchain/langgraph @langchain/openai @langchain/anthropic @langchain/core uuid\n", "```\n", "接下来，我们需要为 Anthropic 和 OpenAI 设置 API 密钥（我们将使用的 LLM 和嵌入）：\n", "```typescript\n", "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n", "process.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n", "```\n", "!!!提示“为 LangGraph 开发设置 [LangSmith](https://smith.langchain.com)”\n", "\n", "注册 LangSmith 以快速发现问题并提高 LangGraph 项目的性能。LangSmith 允许您使用跟踪数据来调试、测试和监控使用 LangGraph 构建的 LLM 应用程序 — 在[此处](https://docs.smith.langchain.com)了解有关如何开始的更多信息"]}, {"cell_type": "markdown", "id": "6b5b3d42-3d2c-455e-ac10-e2ae74dc1cf1", "metadata": {}, "source": ["## 示例：具有长期记忆的简单聊天机器人"]}, {"cell_type": "markdown", "id": "c4c550b5-1954-496b-8b9d-800361af17dc", "metadata": {}, "source": ["### 定义商店\n", "\n", "在此示例中，我们将创建一个能够检索有关用户首选项的信息的工作流程。我们将通过定义一个 `InMemoryStore` 来实现这一点 - 一个可以在内存中存储数据并查询该数据的对象。\n", "\n", "使用 `Store` 接口存储对象时，您定义两件事：\n", "\n", "* 对象的命名空间，一个元组（类似于目录）\n", "* 对象键（类似于文件名）\n", "\n", "在我们的示例中，我们将使用 `[\"memories\", <user_id>]` 作为命名空间，并使用随机 UUID 作为每个新内存的密钥。\n", "\n", "我们首先定义我们的商店："]}, {"cell_type": "code", "execution_count": 1, "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802", "metadata": {}, "outputs": [], "source": ["import { InMemoryStore } from \"@langchain/langgraph\";\n", "import { OpenAIEmbeddings } from \"@langchain/openai\";\n", "\n", "const inMemoryStore = new InMemoryStore({\n", "  index: {\n", "    embeddings: new OpenAIEmbeddings({\n", "      model: \"text-embedding-3-small\",\n", "    }),\n", "    dims: 1536,\n", "  },\n", "});"]}, {"cell_type": "markdown", "id": "3389c9f4-226d-40c7-8bfc-ee8aac24f79d", "metadata": {}, "source": ["### 创建工作流程\n", "\n", "现在让我们创建我们的工作流程："]}, {"cell_type": "code", "execution_count": 2, "id": "2a30a362-528c-45ee-9df6-630d2d843588", "metadata": {}, "outputs": [], "source": ["import { v4 } from \"uuid\";\n", "import { ChatAnthropic } from \"@langchain/anthropic\";\n", "import {\n", "  entrypoint,\n", "  task,\n", "  MemorySaver,\n", "  addMessages,\n", "  type BaseStore,\n", "  getStore,\n", "} from \"@langchain/langgraph\";\n", "import type { BaseMessage, BaseMessageLike } from \"@langchain/core/messages\";\n", "\n", "const model = new ChatAnthropic({\n", "  model: \"claude-3-5-sonnet-latest\",\n", "});\n", "\n", "const callModel = task(\"callModel\", async (\n", "  messages: BaseMessage[],\n", "  memoryStore: BaseStore,\n", "  userId: string\n", ") => {\n", "  const namespace = [\"memories\", userId];\n", "  const lastMessage = messages.at(-1);\n", "  if (typeof lastMessage?.content !== \"string\") {\n", "    throw new Error(\"Received non-string message content.\");\n", "  }\n", "  const memories = await memoryStore.search(namespace, {\n", "    query: lastMessage.content,\n", "  });\n", "  const info = memories.map((memory) => memory.value.data).join(\"\\n\");\n", "  const systemMessage = `You are a helpful assistant talking to the user. User info: ${info}`;\n", "  \n", "  // 如果用户要求模型记住，则存储新的记忆\n", "  if (lastMessage.content.toLowerCase().includes(\"remember\")) {\n", "    // 硬编码用于演示\n", "    const memory = `Username is Bob`;\n", "    await memoryStore.put(namespace, v4(), { data: memory });\n", "  }\n", "  const response = await model.invoke([\n", "    {\n", "      role: \"system\",\n", "      content: systemMessage \n", "    },\n", "    ...messages\n", "  ]);\n", "  return response;\n", "});\n", "\n", "// 注意：当通过entrypoint()创建工作流程时，我们在此处传递商店对象\n", "const workflow = entrypoint({\n", "  checkpointer: new MemorySaver(),\n", "  store: inMemoryStore,\n", "  name: \"workflow\",\n", "}, async (params: {\n", "  messages: BaseMessageLike[];\n", "  userId: string;\n", "}, config) => {\n", "  const messages = addMessages([], params.messages)\n", "  const response = await callModel(messages, config.store, params.userId);\n", "  return entrypoint.final({\n", "    value: response,\n", "    save: addMessages(messages, response),\n", "  });\n", "});"]}, {"cell_type": "markdown", "id": "f22a4a18-67e4-4f0b-b655-a29bbe202e1c", "metadata": {}, "source": ["当前存储作为入口点第二个参数的一部分传入，如 `config.store`。\n", "\n", "!!!注释 注释\n", "\n", "如果您使用 LangGraph Cloud 或 LangGraph Studio，您__不需要__将存储传递到入口点，因为它是自动完成的。"]}, {"cell_type": "markdown", "id": "552d4e33-556d-4fa5-8094-2a076bc21529", "metadata": {}, "source": ["### 运行工作流程！"]}, {"cell_type": "markdown", "id": "1842c626-6cd9-4f58-b549-58978e478098", "metadata": {}, "source": ["现在让我们在配置中指定一个用户 ID 并告诉模型我们的名字："]}, {"cell_type": "code", "execution_count": 3, "id": "c871a073-a466-46ad-aafe-2b870831057e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["AIMessage {\n", "  \"id\": \"msg_01U4xHvf4REPSCGWzpLeh1qJ\",\n", "  \"content\": \"Hi Bob! Nice to meet you. I'll remember that your name is Bob. How can I help you today?\",\n", "  \"additional_kwargs\": {\n", "    \"id\": \"msg_01U4xHvf4REPSCGWzpLeh1qJ\",\n", "    \"type\": \"message\",\n", "    \"role\": \"assistant\",\n", "    \"model\": \"claude-3-5-sonnet-20241022\",\n", "    \"stop_reason\": \"end_turn\",\n", "    \"stop_sequence\": null,\n", "    \"usage\": {\n", "      \"input_tokens\": 28,\n", "      \"cache_creation_input_tokens\": 0,\n", "      \"cache_read_input_tokens\": 0,\n", "      \"output_tokens\": 27\n", "    }\n", "  },\n", "  \"response_metadata\": {\n", "    \"id\": \"msg_01U4xHvf4REPSCGWzpLeh1qJ\",\n", "    \"model\": \"claude-3-5-sonnet-20241022\",\n", "    \"stop_reason\": \"end_turn\",\n", "    \"stop_sequence\": null,\n", "    \"usage\": {\n", "      \"input_tokens\": 28,\n", "      \"cache_creation_input_tokens\": 0,\n", "      \"cache_read_input_tokens\": 0,\n", "      \"output_tokens\": 27\n", "    },\n", "    \"type\": \"message\",\n", "    \"role\": \"assistant\"\n", "  },\n", "  \"tool_calls\": [],\n", "  \"invalid_tool_calls\": [],\n", "  \"usage_metadata\": {\n", "    \"input_tokens\": 28,\n", "    \"output_tokens\": 27,\n", "    \"total_tokens\": 55,\n", "    \"input_token_details\": {\n", "      \"cache_creation\": 0,\n", "      \"cache_read\": 0\n", "    }\n", "  }\n", "}\n"]}], "source": ["const config = {\n", "  configurable: {\n", "    thread_id: \"1\",\n", "  },\n", "  streamMode: \"values\" as const,\n", "};\n", "\n", "const inputMessage = {\n", "  role: \"user\",\n", "  content: \"Hi! Remember: my name is Bob\",\n", "};\n", "\n", "const stream = await workflow.stream({ messages: [inputMessage], userId: \"1\" }, config);\n", "\n", "for await (const chunk of stream) {\n", "  console.log(chunk);\n", "}"]}, {"cell_type": "code", "execution_count": 4, "id": "d862be40-1f8a-4057-81c4-b7bf073dc4c1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["AIMessage {\n", "  \"id\": \"msg_01LB4YapkFawBUbpiu3oeWbF\",\n", "  \"content\": \"Your name is Bob.\",\n", "  \"additional_kwargs\": {\n", "    \"id\": \"msg_01LB4YapkFawBUbpiu3oeWbF\",\n", "    \"type\": \"message\",\n", "    \"role\": \"assistant\",\n", "    \"model\": \"claude-3-5-sonnet-20241022\",\n", "    \"stop_reason\": \"end_turn\",\n", "    \"stop_sequence\": null,\n", "    \"usage\": {\n", "      \"input_tokens\": 28,\n", "      \"cache_creation_input_tokens\": 0,\n", "      \"cache_read_input_tokens\": 0,\n", "      \"output_tokens\": 8\n", "    }\n", "  },\n", "  \"response_metadata\": {\n", "    \"id\": \"msg_01LB4YapkFawBUbpiu3oeWbF\",\n", "    \"model\": \"claude-3-5-sonnet-20241022\",\n", "    \"stop_reason\": \"end_turn\",\n", "    \"stop_sequence\": null,\n", "    \"usage\": {\n", "      \"input_tokens\": 28,\n", "      \"cache_creation_input_tokens\": 0,\n", "      \"cache_read_input_tokens\": 0,\n", "      \"output_tokens\": 8\n", "    },\n", "    \"type\": \"message\",\n", "    \"role\": \"assistant\"\n", "  },\n", "  \"tool_calls\": [],\n", "  \"invalid_tool_calls\": [],\n", "  \"usage_metadata\": {\n", "    \"input_tokens\": 28,\n", "    \"output_tokens\": 8,\n", "    \"total_tokens\": 36,\n", "    \"input_token_details\": {\n", "      \"cache_creation\": 0,\n", "      \"cache_read\": 0\n", "    }\n", "  }\n", "}\n"]}], "source": ["const config2 = {\n", "  configurable: {\n", "    thread_id: \"2\",\n", "  },\n", "  streamMode: \"values\" as const,\n", "};\n", "\n", "const followupStream = await workflow.stream({\n", "  messages: [{\n", "    role: \"user\",\n", "    content: \"what is my name?\",\n", "  }],\n", "  userId: \"1\"\n", "}, config2);\n", "\n", "for await (const chunk of followupStream) {\n", "  console.log(chunk);\n", "}"]}, {"cell_type": "markdown", "id": "80fd01ec-f135-4811-8743-daff8daea422", "metadata": {}, "source": ["我们现在可以检查内存存储并验证我们实际上已经为用户保存了内存："]}, {"cell_type": "code", "execution_count": 5, "id": "76cde493-89cf-4709-a339-207d2b7e9ea7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{ data: 'Username is Bob' }\n"]}], "source": ["const memories = await inMemoryStore.search([\"memories\", \"1\"]);\n", "for (const memory of memories) {\n", "  console.log(memory.value);\n", "}"]}, {"cell_type": "markdown", "id": "23f5d7eb-af23-4131-b8fd-2a69e74e6e55", "metadata": {}, "source": ["现在让我们为另一个用户运行工作流程，以验证有关第一个用户的记忆是否是独立的："]}, {"cell_type": "code", "execution_count": 6, "id": "d362350b-d730-48bd-9652-983812fd7811", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["AIMessage {\n", "  \"id\": \"msg_01KK7CweVY4ZdHxU5bPa4skv\",\n", "  \"content\": \"I don't have any information about your name. While I aim to be helpful, I can only know what you directly tell me during our conversation.\",\n", "  \"additional_kwargs\": {\n", "    \"id\": \"msg_01KK7CweVY4ZdHxU5bPa4skv\",\n", "    \"type\": \"message\",\n", "    \"role\": \"assistant\",\n", "    \"model\": \"claude-3-5-sonnet-20241022\",\n", "    \"stop_reason\": \"end_turn\",\n", "    \"stop_sequence\": null,\n", "    \"usage\": {\n", "      \"input_tokens\": 25,\n", "      \"cache_creation_input_tokens\": 0,\n", "      \"cache_read_input_tokens\": 0,\n", "      \"output_tokens\": 33\n", "    }\n", "  },\n", "  \"response_metadata\": {\n", "    \"id\": \"msg_01KK7CweVY4ZdHxU5bPa4skv\",\n", "    \"model\": \"claude-3-5-sonnet-20241022\",\n", "    \"stop_reason\": \"end_turn\",\n", "    \"stop_sequence\": null,\n", "    \"usage\": {\n", "      \"input_tokens\": 25,\n", "      \"cache_creation_input_tokens\": 0,\n", "      \"cache_read_input_tokens\": 0,\n", "      \"output_tokens\": 33\n", "    },\n", "    \"type\": \"message\",\n", "    \"role\": \"assistant\"\n", "  },\n", "  \"tool_calls\": [],\n", "  \"invalid_tool_calls\": [],\n", "  \"usage_metadata\": {\n", "    \"input_tokens\": 25,\n", "    \"output_tokens\": 33,\n", "    \"total_tokens\": 58,\n", "    \"input_token_details\": {\n", "      \"cache_creation\": 0,\n", "      \"cache_read\": 0\n", "    }\n", "  }\n", "}\n"]}], "source": ["const config3 = {\n", "  configurable: {\n", "    thread_id: \"3\",\n", "  },\n", "  streamMode: \"values\" as const,\n", "};\n", "\n", "const otherUserStream = await workflow.stream({\n", "  messages: [{\n", "    role: \"user\",\n", "    content: \"what is my name?\",\n", "  }],\n", "  userId: \"2\"\n", "}, config3);\n", "\n", "for await (const chunk of otherUserStream) {\n", "  console.log(chunk);\n", "}"]}], "metadata": {"kernelspec": {"display_name": "TypeScript", "language": "typescript", "name": "tslab"}, "language_info": {"codemirror_mode": {"mode": "typescript", "name": "javascript", "typescript": true}, "file_extension": ".ts", "mimetype": "text/typescript", "name": "typescript", "version": "3.7.2"}}, "nbformat": 4, "nbformat_minor": 5}