{"cells": [{"cell_type": "markdown", "id": "3246bf13", "metadata": {}, "source": ["# 如何强制代理调用工具\n", "\n", "在此示例中，我们将构建一个 **始终** 调用某个工具的 ReAct 代理\n", "首先，在制定任何计划之前。在此示例中，我们将创建一个代理\n", "搜索工具。然而，一开始我们将强制代理调用搜索\n", "工具（然后让它做任何它想做的事情）。当您知道时这很有用\n", "您想要在应用程序中执行特定操作，但也希望\n", "让法学硕士在完成后跟进用户的查询的灵活性\n", "那个固定的顺序。"]}, {"cell_type": "markdown", "id": "f7c184d4", "metadata": {}, "source": ["## 设置\n", "\n", "首先我们需要安装需要的包\n", "```bash\n", "yarn add @langchain/langgraph @langchain/openai @langchain/core\n", "```\n", "接下来，我们需要为 OpenAI（我们将使用的 LLM）设置 API 密钥。可选地，我们\n", "可以为[LangSmith追踪](https://smith.langchain.com/)设置API密钥，\n", "将为我们提供一流的可观察性。"]}, {"cell_type": "code", "execution_count": 1, "id": "6327203c", "metadata": {"lines_to_next_cell": 2}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Force Calling a Tool First: LangGraphJS\n"]}], "source": ["// process.env.OPENAI_API_KEY = \"sk_...\";\n", "\n", "// 可选，在 LangSmith 中添加跟踪\n", "// process.env.LANGCHAIN_API_KEY = \"ls__...\";\n", "// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n", "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n", "process.env.LANGCHAIN_PROJECT = \"Force Calling a Tool First: LangGraphJS\";"]}, {"cell_type": "markdown", "id": "247b32e0", "metadata": {}, "source": ["## 设置工具\n", "\n", "我们将首先定义我们想要使用的工具。对于这个简单的例子，我们将\n", "通过 Tavily 使用内置搜索工具。但是，创建您的应用程序确实很容易\n", "自己的工具 - 请参阅文档\n", "[此处](https://js.langchain.com/docs/modules/agents/tools/dynamic)了解如何操作\n", "那。"]}, {"cell_type": "code", "execution_count": 2, "id": "294b9a8c", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["import { DynamicStructuredTool } from \"@langchain/core/tools\";\n", "import { z } from \"zod\";\n", "\n", "const searchTool = new DynamicStructuredTool({\n", "  name: \"search\",\n", "  description:\n", "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n", "  schema: z.object({\n", "    query: z.string().describe(\"The query to use in your search.\"),\n", "  }),\n", "  func: async ({}: { query: string }) => {\n", "    // 这是实际实现的占位符\n", "    return \"Cold, with a low of 13 ℃\";\n", "  },\n", "});\n", "\n", "await searchTool.invoke({ query: \"What's the weather like?\" });\n", "\n", "const tools = [searchTool];"]}, {"cell_type": "markdown", "id": "8aa4e65a", "metadata": {}, "source": ["我们现在可以将这些工具包装在 `ToolNode` 中。\n", "这是一个预先构建的节点，它接收 LangChain 聊天模型生成的工具调用并调用该工具，\n", "返回输出。"]}, {"cell_type": "code", "execution_count": 3, "id": "51927bdc", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n", "\n", "const toolNode = new ToolNode(tools);"]}, {"cell_type": "markdown", "id": "7a2de12a", "metadata": {}, "source": ["## 设置模型\n", "\n", "现在我们需要加载我们想要使用的聊天模型。\\\n", "重要的是，这应该满足两个标准：\n", "\n", "1.它应该与消息一起使用。我们将以表格形式代表所有代理状态\n", "消息，因此它需要能够与它们很好地配合。\n", "2. 需要配合OpenAI函数调用。这意味着它应该是\n", "OpenAI 模型或公开类似接口的模型。\n", "\n", "注意：这些模型要求不是使用 LangGraph 的要求 - 它们\n", "只是这个例子的要求。"]}, {"cell_type": "code", "execution_count": 4, "id": "d6e85df1", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["import { ChatOpenAI } from \"@langchain/openai\";\n", "\n", "const model = new ChatOpenAI({\n", "  temperature: 0,\n", "  model: \"gpt-4o\",\n", "});"]}, {"cell_type": "markdown", "id": "97cfea7d", "metadata": {}, "source": ["完成此操作后，我们应该确保模型知道它具有这些\n", "可供调用的工具。我们可以通过将 LangChain 工具转换为\n", "OpenAI函数调用的格式，然后将它们绑定到模型类。"]}, {"cell_type": "code", "execution_count": 5, "id": "4f0b4a08", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["const boundModel = model.bindTools(tools);"]}, {"cell_type": "markdown", "id": "59ca3d78", "metadata": {}, "source": ["## 定义代理状态\n", "\n", "`langgraph` 中图形的主要类型是 `StateGraph`。该图是\n", "由传递到每个节点的状态对象参数化。每个节点\n", "然后返回更新该状态的操作。\n", "\n", "对于这个例子，我们将跟踪的状态只是一个消息列表。我们\n", "希望每个节点只将消息添加到该列表中。因此，我们将定义\n", "代理状态作为一个具有一个键（`messages`）的对象，其值指定如何\n", "更新状态。"]}, {"cell_type": "code", "execution_count": 6, "id": "3db3dd6e", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["import { Annotation } from \"@langchain/langgraph\";\n", "import { BaseMessage } from \"@langchain/core/messages\";\n", "\n", "const AgentState = Annotation.Root({\n", "  messages: Annotation<BaseMessage[]>({\n", "    reducer: (x, y) => x.concat(y),\n", "  }),\n", "});"]}, {"cell_type": "markdown", "id": "97b540fc", "metadata": {}, "source": ["## 定义节点\n", "\n", "我们现在需要在图中定义一些不同的节点。在`langgraph`中，一个节点\n", "可以是一个函数或一个\n", "[可运行](https://js.langchain.com/docs/expression_language/)。有两个\n", "为此我们需要的主要节点：\n", "\n", "1. 代理：负责决定采取什么（如果有）行动。\n", "2. 调用工具的函数：如果代理决定采取行动，该节点\n", "然后将执行该操作。\n", "\n", "我们还需要定义一些边。其中一些边缘可能是有条件的。\n", "它们是有条件的原因是基于节点的输出，其中之一\n", "可以采取多条路径。直到该节点才知道所采取的路径\n", "运行（LLM 决定）。\n", "\n", "1. 条件边缘：调用代理后，我们应该：如果\n", "代理说要采取行动，那么调用工具的函数应该是\n", "称为\\\n", "b.如果代理说已经完成，那么就应该完成\n", "2. 正常边缘：调用工具后，应始终返回到\n", "代理决定下一步做什么\n", "\n", "让我们定义节点以及一个函数来决定什么条件\n", "边取。"]}, {"cell_type": "code", "execution_count": 7, "id": "eee5adc0", "metadata": {}, "outputs": [], "source": ["import { AIMessage, AIMessageChunk } from \"@langchain/core/messages\";\n", "import { RunnableConfig } from \"@langchain/core/runnables\";\n", "import { concat } from \"@langchain/core/utils/stream\";\n", "\n", "// 定义将用于确定要下降的条件边沿的逻辑\n", "const shouldContinue = (state: typeof AgentState.State) => {\n", "  const { messages } = state;\n", "  const lastMessage = messages[messages.length - 1] as AIMessage;\n", "  // 如果没有函数调用，那么我们就完成了\n", "  if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) {\n", "    return \"end\";\n", "  }\n", "  // 否则，如果有，我们继续\n", "  return \"continue\";\n", "};\n", "\n", "// 定义调用模型的函数\n", "const callModel = async (\n", "  state: typeof AgentState.State,\n", "  config?: RunnableConfig,\n", ") => {\n", "  const { messages } = state;\n", "  let response: AIMessageChunk | undefined;\n", "  for await (const message of await boundModel.stream(messages, config)) {\n", "    if (!response) {\n", "      response = message;\n", "    } else {\n", "      response = concat(response, message);\n", "    }\n", "  }\n", "  // 我们返回一个对象，因为这将被添加到现有列表中\n", "  return {\n", "    messages: response ? [response as AIMessage] : [],\n", "  };\n", "};"]}, {"cell_type": "markdown", "id": "00a87145", "metadata": {}, "source": ["**修改**\n", "\n", "在这里，我们创建一个通过工具调用返回 AIMessage 的节点 - 我们将使用\n", "这在开始时强制它调用一个工具"]}, {"cell_type": "code", "execution_count": 8, "id": "e9104fc7", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["// 这是新的第一次 - 我们想要显式硬编码某些操作的模型的第一次调用\n", "const firstModel = async (state: typeof AgentState.State) => {\n", "  const humanInput = state.messages[state.messages.length - 1].content || \"\";\n", "  return {\n", "    messages: [\n", "      new AIMessage({\n", "        content: \"\",\n", "        tool_calls: [\n", "          {\n", "            name: \"search\",\n", "            args: {\n", "              query: humanInput,\n", "            },\n", "            id: \"tool_abcd123\",\n", "          },\n", "        ],\n", "      }),\n", "    ],\n", "  };\n", "};"]}, {"cell_type": "markdown", "id": "e609be17", "metadata": {}, "source": ["## 定义图表\n", "\n", "我们现在可以将它们放在一起并定义图表！\n", "\n", "**修改**\n", "\n", "我们将定义一个 `firstModel` 节点，将其设置为入口点。\n"]}, {"cell_type": "code", "execution_count": 9, "id": "de6da918", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["import { END, START, StateGraph } from \"@langchain/langgraph\";\n", "\n", "// 定义一个新图\n", "const workflow = new StateGraph(AgentState)\n", "  // 定义新的入口点\n", "  .addNode(\"first_agent\", firstModel)\n", "  // 定义我们将在其之间循环的两个节点\n", "  .addNode(\"agent\", callModel)\n", "  .addNode(\"action\", toolNode)\n", "  // 将入口点设置为“first_agent”\n", "  // 通过创建从虚拟 __start__ 节点到 `first_agent` 的边\n", "  .addEdge(START, \"first_agent\")\n", "  // 我们现在添加一个条件边\n", "  .addConditionalEdges(\n", "    // 首先，我们定义起始节点。我们使用“代理”。\n", "    // 这意味着这些是调用“agent”节点后获取的边。\n", "    \"agent\",\n", "    // 接下来，我们传入将确定接下来调用哪个节点的函数。\n", "    shouldContinue,\n", "    // 最后我们传入一个映射。\n", "    // 键是字符串，值是其他节点。\n", "    // END 是一个特殊的节点，标记图形应该结束。\n", "    // 将会发生的事情是我们将调用“should_continue”，然后该输出\n", "    // 将与此映射中的键进行匹配。\n", "    // 根据匹配的节点，将调用该节点。\n", "    {\n", "      // 如果是“tools”，那么我们称之为工具节点。\n", "      continue: \"action\",\n", "      // 否则我们就结束了。\n", "      end: END,\n", "    },\n", "  )\n", "  // 我们现在添加从“tools”到“agent”的正常边缘。\n", "  // 这意味着在调用“tools”之后，接下来调用“agent”节点。\n", "  .addEdge(\"action\", \"agent\")\n", "  // 在我们致电第一个代理后，我们知道我们想要采取行动\n", "  .addEdge(\"first_agent\", \"action\");\n", "\n", "// 最后，我们编译它！\n", "// 这会将其编译成 LangChain Runnable，\n", "// 这意味着您可以像使用任何其他可运行程序一样使用它\n", "const app = workflow.compile();"]}, {"cell_type": "markdown", "id": "bd4f83be", "metadata": {}, "source": ["## 使用它！\n", "\n", "我们现在可以使用它了！现在这暴露了\n", "[相同的界面](https://js.langchain.com/docs/expression_language/) 与所有\n", "其他 LangChain 运行程序。"]}, {"cell_type": "code", "execution_count": 10, "id": "acaade41", "metadata": {"lines_to_next_cell": 2}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{\n", "  first_agent: {\n", "    messages: [\n", "      AIMessage {\n", "        \"content\": \"\",\n", "        \"additional_kwargs\": {},\n", "        \"response_metadata\": {},\n", "        \"tool_calls\": [\n", "          {\n", "            \"name\": \"search\",\n", "            \"args\": {\n", "              \"query\": \"what is the weather in sf\"\n", "            },\n", "            \"id\": \"tool_abcd123\"\n", "          }\n", "        ],\n", "        \"invalid_tool_calls\": []\n", "      }\n", "    ]\n", "  }\n", "}\n", "-----\n", "\n", "{\n", "  action: {\n", "    messages: [\n", "      ToolMessage {\n", "        \"content\": \"Cold, with a low of 13 ℃\",\n", "        \"name\": \"search\",\n", "        \"additional_kwargs\": {},\n", "        \"response_metadata\": {},\n", "        \"tool_call_id\": \"tool_abcd123\"\n", "      }\n", "    ]\n", "  }\n", "}\n", "-----\n", "\n", "{\n", "  agent: {\n", "    messages: [\n", "      AIMessageChunk {\n", "        \"id\": \"chatcmpl-9y562g16z0MUNBJcS6nKMsDuFMRsS\",\n", "        \"content\": \"The current weather in San Francisco is cold, with a low of 13°C.\",\n", "        \"additional_kwargs\": {},\n", "        \"response_metadata\": {\n", "          \"prompt\": 0,\n", "          \"completion\": 0,\n", "          \"finish_reason\": \"stop\",\n", "          \"system_fingerprint\": \"fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27fp_3aa7262c27\"\n", "        },\n", "        \"tool_calls\": [],\n", "        \"tool_call_chunks\": [],\n", "        \"invalid_tool_calls\": [],\n", "        \"usage_metadata\": {\n", "          \"input_tokens\": 104,\n", "          \"output_tokens\": 18,\n", "          \"total_tokens\": 122\n", "        }\n", "      }\n", "    ]\n", "  }\n", "}\n", "-----\n", "\n"]}], "source": ["import { HumanMessage } from \"@langchain/core/messages\";\n", "\n", "const inputs = {\n", "  messages: [new HumanMessage(\"what is the weather in sf\")],\n", "};\n", "\n", "for await (const output of await app.stream(inputs)) {\n", "  console.log(output);\n", "  console.log(\"-----\\n\");\n", "}"]}], "metadata": {"jupytext": {"encoding": "# -*- coding: utf-8 -*-"}, "kernelspec": {"display_name": "TypeScript", "language": "typescript", "name": "tslab"}, "language_info": {"codemirror_mode": {"mode": "typescript", "name": "javascript", "typescript": true}, "file_extension": ".ts", "mimetype": "text/typescript", "name": "typescript", "version": "3.7.2"}}, "nbformat": 4, "nbformat_minor": 5}